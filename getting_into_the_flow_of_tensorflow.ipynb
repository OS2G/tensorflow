{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getting_into_the_flow_of_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBHgx2MFjQVJ"
      },
      "source": [
        "# Getting into the flow of TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osjwS5Utjar8"
      },
      "source": [
        "Jupyter notebooks are simply runtime blocks of python code that can interact with eachother. It's a great tool if you're just trying some stuff out with python, you're trying to document a process, or you want an easy to use and visual system to run your python code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2foZD3y6j2Lf"
      },
      "source": [
        "They are composed of markdown blocks and code blocks. This allows for easy stylized documentation.\n",
        "\n",
        "Code blocks are executed with the \"play\" button to the left of the block where the brackets are. Once a block is executed, the output stays in memory and any prints, graphs, images, or tables are saved to the notebook itself. This means that a variable or function defined in another block will be visible to all blocks once the block has been executed. This is both useful and can be a pain since everything that's outside of a function global, as if this is one large python script.\n",
        "\n",
        "You can always rerun a block of code if you're changed things. The outputs will be overwritten and related variables updated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYSCOcScmN5M"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DanodEwJmRd4"
      },
      "source": [
        "Libraries can be imported anywhere within your code, but often times, people do so at the top of script so that you have everything you need when working.\n",
        "\n",
        "You can import libraries as is with a simple `import library`. But often it's useful to abreviate so that you don't need to type out the full name like `import lib`. You can also import a function or sub-library like:\n",
        "`import lib.sublib as slib` or `from lib.sublib import function as func` which will let you use `func()` anwhere in your code then."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrVEo-QYi9NL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-QSKMG8nscd"
      },
      "source": [
        "## Datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_18AvxZjn5zb"
      },
      "source": [
        "When doing machine learning, you need a dataset to work with for training and testing. This can be images, text, datapoints in some format like csv, etc. In this case, we will be importing the MNIST handwriting dataset where we will try to build a recognizer that will recognize different handwritten numbers as their respective number.\n",
        "\n",
        "We first import the data set and then during the load step, we get a split of the data with training samples and testing samples. This is normally an 80/20 split.\n",
        "\n",
        "The X matrix is composed of \"features\" which describe the data and the Y matrix (or vector) is composed of \"labels\" which describe what feature we are trying to predict.\n",
        "\n",
        "In this case, we are working with images that have been converted to matrices of pixels. By default, the values are white color values from 0 to 255 where 0 is black and 255 is white. Everything in between is a form of grey.\n",
        "\n",
        "We load the data into X and Y training sets, both are subsets of the full set. We also get an X and Y test set which is a smaller subset of the full set. Both of these sets are independent, they have no overlapping samples which is key to measuring the performance of your model (machine learning system).\n",
        "\n",
        "We then normalize the color values to be floats of range [0., 1.], allowing tensorflow to work with the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBJJdAWWj0PF"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9751O8VxtypF"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(x_train[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilUyb4JXjMdo"
      },
      "source": [
        "## Training\n",
        "\n",
        "A model is a machine learning algorithm with a certain set of parameters and a specific data set it trains on. Once it trains, it has a set of 'weights' which make up the model's parameters which is then used for predicting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1vd81masTno"
      },
      "source": [
        "We start by gibing some parameters to tensorflow's keras algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5izDmk6Bsxso"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyeHr0XgsrPk"
      },
      "source": [
        "We then train the model on a single input, at the same time getting predictions on what class it may be"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjN0ZYhYsynX"
      },
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB_E9luKsfax"
      },
      "source": [
        "We then use softmax regression as our activation function to classify the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qc3YVRss5aa"
      },
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZm1Mn0osunv"
      },
      "source": [
        "We get a loss function to test our training loss for an itteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L67tDsxWtALB"
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-jpnZgBsz9t"
      },
      "source": [
        "We then compare our prediction to the expected class and get a loss or error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_CSgFQGtC80"
      },
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlNpqsd6tMC0"
      },
      "source": [
        "Now we compile our model to make it ready for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXVCk9VgtFSg"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcfNwi8ZtPYN"
      },
      "source": [
        "Now we train the full set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEwJx2QRtItQ"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxbwk4ZktQ_v"
      },
      "source": [
        "Here we evaluate on the test set, the accuracy and loss are shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izeg7vbDtNOF"
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VenMVIFstPcj"
      },
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4AN1pdatWsU"
      },
      "source": [
        "probability_model(x_test[:5])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}